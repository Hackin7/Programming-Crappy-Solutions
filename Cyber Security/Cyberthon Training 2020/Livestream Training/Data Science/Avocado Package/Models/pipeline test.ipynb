{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avocado Data Analysis Notebook\n",
    "ALT-TAB LABS LLP &copy; 2020 All Rights Reserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['.ipynb_checkpoints',\n 'avocado-format.png',\n 'avocado-submission.csv',\n 'avocado-test.csv',\n 'avocado-train.csv',\n 'avocado_deep model.ipynb',\n 'avocado_notebook.ipynb',\n 'avocado_notebook_other_models.ipynb',\n 'Given',\n 'Models',\n 'pyctfsglib.py',\n '__pycache__']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show files in current directory\n",
    "import os\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>DataBatch</th>\n      <th>Date</th>\n      <th>TotalVolume</th>\n      <th>4046</th>\n      <th>4225</th>\n      <th>4770</th>\n      <th>TotalBags</th>\n      <th>SmallBags</th>\n      <th>LargeBags</th>\n      <th>XLargeBags</th>\n      <th>type</th>\n      <th>region</th>\n      <th>AveragePrice</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7</td>\n      <td>2016-11-06</td>\n      <td>183542.31</td>\n      <td>98949.98</td>\n      <td>22891.61</td>\n      <td>95.00</td>\n      <td>61605.72</td>\n      <td>43571.99</td>\n      <td>17499.01</td>\n      <td>534.72</td>\n      <td>conventional</td>\n      <td>NewOrleansMobile</td>\n      <td>1.49</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24</td>\n      <td>2017-07-16</td>\n      <td>224434.92</td>\n      <td>42951.31</td>\n      <td>120360.02</td>\n      <td>131.85</td>\n      <td>60991.74</td>\n      <td>53141.81</td>\n      <td>3621.04</td>\n      <td>4228.89</td>\n      <td>conventional</td>\n      <td>HarrisburgScranton</td>\n      <td>1.38</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>51</td>\n      <td>2015-01-04</td>\n      <td>3846.69</td>\n      <td>1500.15</td>\n      <td>938.35</td>\n      <td>0.00</td>\n      <td>1408.19</td>\n      <td>1071.35</td>\n      <td>336.84</td>\n      <td>0.00</td>\n      <td>organic</td>\n      <td>Atlanta</td>\n      <td>1.76</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>22</td>\n      <td>2015-07-26</td>\n      <td>91825.07</td>\n      <td>1679.28</td>\n      <td>45615.48</td>\n      <td>741.77</td>\n      <td>43788.54</td>\n      <td>43788.54</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>conventional</td>\n      <td>BuffaloRochester</td>\n      <td>1.39</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50</td>\n      <td>2015-01-11</td>\n      <td>54644.32</td>\n      <td>1491.88</td>\n      <td>33759.12</td>\n      <td>1325.17</td>\n      <td>18068.15</td>\n      <td>12165.94</td>\n      <td>5902.21</td>\n      <td>0.00</td>\n      <td>conventional</td>\n      <td>Pittsburgh</td>\n      <td>1.54</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "    DataBatch        Date  TotalVolume      4046       4225     4770  \\\nid                                                                     \n0           7  2016-11-06    183542.31  98949.98   22891.61    95.00   \n1          24  2017-07-16    224434.92  42951.31  120360.02   131.85   \n2          51  2015-01-04      3846.69   1500.15     938.35     0.00   \n3          22  2015-07-26     91825.07   1679.28   45615.48   741.77   \n4          50  2015-01-11     54644.32   1491.88   33759.12  1325.17   \n\n    TotalBags  SmallBags  LargeBags  XLargeBags          type  \\\nid                                                              \n0    61605.72   43571.99   17499.01      534.72  conventional   \n1    60991.74   53141.81    3621.04     4228.89  conventional   \n2     1408.19    1071.35     336.84        0.00       organic   \n3    43788.54   43788.54       0.00        0.00  conventional   \n4    18068.15   12165.94    5902.21        0.00  conventional   \n\n                region  AveragePrice  \nid                                    \n0     NewOrleansMobile          1.49  \n1   HarrisburgScranton          1.38  \n2              Atlanta          1.76  \n3     BuffaloRochester          1.39  \n4           Pittsburgh          1.54  "
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load csvs\n",
    "import pandas as pd\n",
    "df = pd.read_csv('avocado-train.csv', index_col='id')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "training split:  8558 ; test split:  4216\n"
    }
   ],
   "source": [
    "# Convert strings to numbers\n",
    "c = ['TotalVolume',\"4046\",\"4225\",\"4770\",\"TotalBags\",\"SmallBags\",\"LargeBags\",\"XLargeBags\",\"AveragePrice\"]\n",
    "for column in c:\n",
    "    df[column] = pd.to_numeric(df[column])\n",
    "\n",
    "# Select data for learning\n",
    "features = c[:-1]+[\"type\",\"region\"]\n",
    "X = df[features]\n",
    "Y = df[\"AveragePrice\"]\n",
    "\n",
    "# Split training into some for training and some for testing\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print(\"training split: \", len(X_train), \"; test split: \", len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dealing with Missing Values ########################################################\n",
    "# https://www.kaggle.com/alexisbcook/missing-values\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "def score_dataset(X_train, X_valid, y_train, y_valid):\n",
    "    model = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_valid)\n",
    "    return mean_absolute_error(y_valid, preds)\n",
    "\n",
    "def dropColumns(X_train, X_valid):\n",
    "    # Get names of columns with missing values\n",
    "    cols_with_missing = [col for col in X_train.columns\n",
    "                        if X_train[col].isnull().any()]\n",
    "\n",
    "    # Drop columns in training and validation data\n",
    "    reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "    reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "    return reduced_X_train, reduced_X_valid\n",
    "\n",
    "\n",
    "def imputing(X_train, X_valid):\n",
    "    # Imputation\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    my_imputer = SimpleImputer()\n",
    "    imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "    imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_X_train.columns = X_train.columns\n",
    "    imputed_X_valid.columns = X_valid.columns\n",
    "    return imputed_X_train, imputed_X_valid\n",
    "\n",
    "def imputePlus(X_train, X_valid):\n",
    "    X_train_plus = X_train.copy()\n",
    "    X_valid_plus = X_valid.copy()\n",
    "\n",
    "    # Make new columns indicating what will be imputed\n",
    "    # Get names of columns with missing values\n",
    "    cols_with_missing = [col for col in X_train.columns\n",
    "                        if X_train[col].isnull().any()]\n",
    "    for col in cols_with_missing:\n",
    "        X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "        X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "    # Imputation\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    my_imputer = SimpleImputer()\n",
    "    imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "    imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "    # Imputation removed column names; put them back\n",
    "    imputed_X_train_plus.columns = X_train_plus.columns\n",
    "    imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "    return imputed_X_train_plus, imputed_X_valid_plus\n",
    "\n",
    "### Dealing with Categorial Variables ##################################################\n",
    "def getCategories(X_train):\n",
    "    # Get list of categorical variables\n",
    "    s = (X_train.dtypes == 'object')\n",
    "    object_cols = list(s[s].index)\n",
    "    return object_cols\n",
    "\n",
    "def dropCategories(X_train, X_valid):\n",
    "    object_cols = getCategories(X_train)\n",
    "    drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "    drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "    return drop_X_train, drop_X_valid, Y_train, Y_valid\n",
    "\n",
    "# Label Encoding\n",
    "def labelEncoding(X_train, X_valid):\n",
    "    object_cols = getCategories(X_train)\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    # Make copy to avoid changing original data \n",
    "    label_X_train = X_train.copy()\n",
    "    label_X_valid = X_valid.copy()\n",
    "\n",
    "    # Apply label encoder to each column with categorical data\n",
    "    label_encoder = LabelEncoder()\n",
    "    for col in object_cols:\n",
    "        label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "        label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "    return label_X_train, label_X_valid, Y_train, Y_valid\n",
    "\n",
    "### One Hot Encoding\n",
    "def oneHotEncoding(X_train, X_valid, debug=True):\n",
    "    object_cols = getCategories(X_train)\n",
    "\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    # Apply one-hot encoder to each column with categorical data\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "    OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "    # One-hot encoding removed index; put it back\n",
    "    OH_cols_train.index = X_train.index\n",
    "    OH_cols_valid.index = X_valid.index\n",
    "\n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    num_X_train = X_train.drop(object_cols, axis=1)\n",
    "    num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "    OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "    return OH_X_train, OH_X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data!\n",
    "#df.head()\n",
    "X_train, X_test = oneHotEncoding(X_train, X_test)\n",
    "X_train, X_test = imputePlus(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>TotalVolume</th>\n      <th>4046</th>\n      <th>4225</th>\n      <th>4770</th>\n      <th>TotalBags</th>\n      <th>SmallBags</th>\n      <th>LargeBags</th>\n      <th>XLargeBags</th>\n      <th>0</th>\n      <th>1</th>\n      <th>...</th>\n      <th>46</th>\n      <th>47</th>\n      <th>48</th>\n      <th>49</th>\n      <th>50</th>\n      <th>51</th>\n      <th>52</th>\n      <th>53</th>\n      <th>54</th>\n      <th>55</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>174079.23</td>\n      <td>1225.77</td>\n      <td>102134.85</td>\n      <td>3082.60</td>\n      <td>67636.01</td>\n      <td>60761.60</td>\n      <td>6851.61</td>\n      <td>22.80</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24727.46</td>\n      <td>201.46</td>\n      <td>24405.82</td>\n      <td>0.00</td>\n      <td>120.18</td>\n      <td>30.00</td>\n      <td>90.18</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5570915.26</td>\n      <td>2780859.66</td>\n      <td>2108450.36</td>\n      <td>121614.31</td>\n      <td>559990.93</td>\n      <td>520299.26</td>\n      <td>36501.18</td>\n      <td>3190.49</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>634.09</td>\n      <td>228.39</td>\n      <td>24.04</td>\n      <td>0.00</td>\n      <td>381.66</td>\n      <td>381.66</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2591981.76</td>\n      <td>1014382.60</td>\n      <td>1205553.71</td>\n      <td>74862.42</td>\n      <td>297183.03</td>\n      <td>280841.88</td>\n      <td>13203.35</td>\n      <td>3137.80</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 64 columns</p>\n</div>",
      "text/plain": "   TotalVolume        4046        4225       4770  TotalBags  SmallBags  \\\n0    174079.23     1225.77   102134.85    3082.60   67636.01   60761.60   \n1     24727.46      201.46    24405.82       0.00     120.18      30.00   \n2   5570915.26  2780859.66  2108450.36  121614.31  559990.93  520299.26   \n3       634.09      228.39       24.04       0.00     381.66     381.66   \n4   2591981.76  1014382.60  1205553.71   74862.42  297183.03  280841.88   \n\n   LargeBags  XLargeBags    0    1  ...   46   47   48   49   50   51   52  \\\n0    6851.61       22.80  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n1      90.18        0.00  0.0  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n2   36501.18     3190.49  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n3       0.00        0.00  0.0  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n4   13203.35     3137.80  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n\n    53   54   55  \n0  0.0  0.0  0.0  \n1  0.0  0.0  0.0  \n2  0.0  0.0  0.0  \n3  0.0  0.0  0.0  \n4  0.0  0.0  0.0  \n\n[5 rows x 64 columns]"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()\n",
    "#y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "training split:  8558 ; test split:  4216\nMAE: 0.14552938804554078\n"
    }
   ],
   "source": [
    "# Preprocessing of training data, fit model \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_full, X_test_full, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print(\"training split: \", len(X_train), \"; test split: \", len(X_test))\n",
    "# \"Cardinality\" means the number of unique values in a column\n",
    "# Select categorical columns with relatively low cardinality (convenient but arbitrary)\n",
    "categorical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].nunique() < 10 and \n",
    "                        X_train_full[cname].dtype == \"object\"]\n",
    "# Select numerical columns\n",
    "numerical_cols = [cname for cname in X_train_full.columns if X_train_full[cname].dtype in ['int64', 'float64']]\n",
    "# Keep selected columns only\n",
    "my_cols = categorical_cols + numerical_cols\n",
    "X_train = X_train_full[my_cols].copy()\n",
    "X_test = X_test_full[my_cols].copy()\n",
    "##############################################################################################################\n",
    "# Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "numerical_transformer = SimpleImputer(strategy='constant')\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Bundle preprocessing for numerical and categorical data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                              ('model', model)\n",
    "                             ])\n",
    "\n",
    "my_pipeline.fit(X_train, Y_train)\n",
    "\n",
    "# Preprocessing of validation data, get predictions\n",
    "preds = my_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(Y_test, preds)\n",
    "print('MAE:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sci Kit Learn! Scientific Computing library for python\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n                      max_features=None, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=2,\n                      min_weight_fraction_leaf=0.0, presort='deprecated',\n                      random_state=2020, splitter='best')\nRandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=None, oob_score=False,\n                      random_state=2020, verbose=0, warm_start=False)\n"
    },
    {
     "data": {
      "text/plain": "'\\nfrom sklearn.preprocessing import StandardScaler\\nscaler = StandardScaler()\\n# Fit only to the training data\\nscaler.fit(Xtrain)\\nXtrain = scaler.transform(Xtrain)\\nXtest = scaler.transform(Xtest)\\n\\nfrom sklearn.neural_network import MLPClassifier\\nscaler = StandardScaler()\\nmodel = MLPClassifier(hidden_layer_sizes=(len(features),len(features),len(features)))\\n'"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the regression model we want to use\n",
    "\n",
    "import sklearn.tree as tree\n",
    "models = [\n",
    "    tree.DecisionTreeRegressor(random_state=2020),\n",
    "    ensemble.RandomForestRegressor(random_state=2020, n_estimators=100)\n",
    "]\n",
    "\n",
    "currScore = 0\n",
    "model = None\n",
    "for i in models:\n",
    "    print(i)\n",
    "    i.fit(X_train,Y_train)\n",
    "    score = i.score(X_test,Y_test)\n",
    "    if score > currScore:\n",
    "        model = i\n",
    "        currScore = score\n",
    "\n",
    "# https://www.kdnuggets.com/2016/10/beginners-guide-neural-networks-python-scikit-learn.html/2\n",
    "'''\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(Xtrain)\n",
    "Xtrain = scaler.transform(Xtrain)\n",
    "Xtest = scaler.transform(Xtest)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "scaler = StandardScaler()\n",
    "model = MLPClassifier(hidden_layer_sizes=(len(features),len(features),len(features)))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.7968415357595394"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score model\n",
    "model.score(X_test, Y_test) #TODO TODO TODO TODO TODO TODO TODO TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "oneHotEncoding() takes from 2 to 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-7253beaaab03>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moneHotEncoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimputePlus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: oneHotEncoding() takes from 2 to 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Clean Data\n",
    "MyDataFrame = pd.read_csv('avocado-test.csv', index_col='id')\n",
    "# Drops missing values \n",
    "#MyDataFrame.dropna()\n",
    "# Convert strings to numbers\n",
    "c = ['TotalVolume',\"4046\",\"4225\",\"4770\",\"TotalBags\",\"SmallBags\",\"LargeBags\",\"XLargeBags\"]#,\"AveragePrice\"]\n",
    "for column in c:\n",
    "    MyDataFrame[column] = pd.to_numeric(MyDataFrame[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>type</th>\n      <th>TotalVolume</th>\n      <th>4046</th>\n      <th>4225</th>\n      <th>4770</th>\n      <th>TotalBags</th>\n      <th>SmallBags</th>\n      <th>LargeBags</th>\n      <th>XLargeBags</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10284</th>\n      <td>organic</td>\n      <td>1573.28</td>\n      <td>3.71</td>\n      <td>39.58</td>\n      <td>0.00</td>\n      <td>1529.99</td>\n      <td>1529.99</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>5461</th>\n      <td>conventional</td>\n      <td>29569086.56</td>\n      <td>9422275.27</td>\n      <td>13846044.56</td>\n      <td>1112405.42</td>\n      <td>5188361.31</td>\n      <td>3938274.51</td>\n      <td>1183732.08</td>\n      <td>66354.72</td>\n    </tr>\n    <tr>\n      <th>1010</th>\n      <td>conventional</td>\n      <td>79513.63</td>\n      <td>6624.37</td>\n      <td>63384.56</td>\n      <td>37.48</td>\n      <td>9467.22</td>\n      <td>8958.14</td>\n      <td>509.08</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2189</th>\n      <td>organic</td>\n      <td>4883.55</td>\n      <td>408.44</td>\n      <td>3284.07</td>\n      <td>0.00</td>\n      <td>1191.04</td>\n      <td>572.85</td>\n      <td>618.19</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2995</th>\n      <td>organic</td>\n      <td>25757.73</td>\n      <td>3749.12</td>\n      <td>11950.72</td>\n      <td>63.86</td>\n      <td>9994.03</td>\n      <td>647.47</td>\n      <td>9346.56</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "               type  TotalVolume        4046         4225        4770  \\\nid                                                                      \n10284       organic      1573.28        3.71        39.58        0.00   \n5461   conventional  29569086.56  9422275.27  13846044.56  1112405.42   \n1010   conventional     79513.63     6624.37     63384.56       37.48   \n2189        organic      4883.55      408.44      3284.07        0.00   \n2995        organic     25757.73     3749.12     11950.72       63.86   \n\n        TotalBags   SmallBags   LargeBags  XLargeBags  \nid                                                     \n10284     1529.99     1529.99        0.00        0.00  \n5461   5188361.31  3938274.51  1183732.08    66354.72  \n1010      9467.22     8958.14      509.08        0.00  \n2189      1191.04      572.85      618.19        0.00  \n2995      9994.03      647.47     9346.56        0.00  "
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select features\n",
    "XforPredictions =  MyDataFrame[my_cols]#features]#None #TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "\n",
    "#my_pipeline.fit(X_train, Y_train)\n",
    "# Preprocessing of validation data, get predictions\n",
    "\n",
    "\n",
    "#X_train, X_test = oneHotEncoding(X, XforPredictions)\n",
    "#X_train, X_test = imputePlus(X_train ,X_test)\n",
    "\n",
    "#XforPredictions.head()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Predictions: [1.9344 1.4678 1.5374 ... 1.4249 1.8776 1.6919]\n"
    }
   ],
   "source": [
    "# Make Predictions!\n",
    "#model.fit(X,Y)\n",
    "yPredictions =  my_pipeline.predict(XforPredictions)# model.predict(X_test)# None #TODO TODO TODO TODO TODO TODO TODO TODO\n",
    "print(\"Predictions:\", yPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "id  AveragePrice\n0        0        1.9344\n1        1        1.4678\n2        2        1.5374\n3        3        1.5029\n4        4        1.4265\n...    ...           ...\n5470  5470        1.5770\n5471  5471        1.3620\n5472  5472        1.4249\n5473  5473        1.8776\n5474  5474        1.6919\n\n[5475 rows x 2 columns]\n"
    }
   ],
   "source": [
    "# Save to CSV File!\n",
    "XforPredictions =  MyDataFrame[features]\n",
    "output = pd.DataFrame({'id': XforPredictions.index, 'AveragePrice': yPredictions})\n",
    "output.to_csv('avocado-submission.csv', index=False)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload for grading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Downloaded pyctfsglib.py: True\n"
    }
   ],
   "source": [
    "# Download CTFSG Grader Libraries\n",
    "import urllib.request, os\n",
    "urllib.request.urlretrieve('https://raw.githubusercontent.com/alttablabs/ctfsg-utils/master/pyctfsglib.py', './pyctfsglib.py')\n",
    "print('Downloaded pyctfsglib.py:', 'pyctfsglib.py' in os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DSGraderClient: Successfully Connected!\n[SERVER] MOTD: CHECK your USER_TOKEN and GRADER_URL HTTP address! I'm AVOCADO_PRICE TEST_GRADER_1\n"
    }
   ],
   "source": [
    "# Connect to graders\n",
    "import pyctfsglib as ctfsg\n",
    "import random\n",
    "\n",
    "USER_TOKEN = \"MXhtGfdjdsUfiEKTHHEuVGohZESBdMiHrFkmYqNqIFfcWOHGvcubvHJvnxpAqRMh\" # You need to fill this up\n",
    "GRADER_URL = random.choice([\n",
    "\"http://challenges.csdc20t.ctf.sg:30001/\", \"http://challenges.csdc20t.ctf.sg:30002/\"\n",
    "])\n",
    "grader = ctfsg.DSGraderClient(GRADER_URL, USER_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ProofOfWork Challenge =>  ('CTFSGRB155394a824a80106bc53160d69a2e984', 22)\nProofOfWork Answer Found! =>  4815595\n"
    },
    {
     "data": {
      "text/plain": "'{\"challenge\":{\"name\":\"Avocado Prices\"},\"id\":\"ck8mqfgke6u520868usyn9kvg\",\"status\":\"PARTIALLY_CORRECT\",\"multiplier\":0.7539,\"submittedBy\":{\"username\":\"nyjc-1\"},\"createdAt\":\"2020-04-05T07:36:27Z\"}'"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.submitFile('avocado-submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}